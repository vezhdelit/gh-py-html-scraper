name: Run sb.py script

on:
  workflow_dispatch:
    inputs:
      url:
        description: 'URL to scrape'
        required: true
      webhook_url:
        description: 'Webhook URL to send results'
        required: true
        default: 'https://backend.vezhdelit.com/api/webhook/scraped-html'
      proxy:
        description: 'Proxy to use for scraping. Format: user:password@host:port or host:port'
        required: false
        default: ''

jobs:
  build:
    runs-on: ubuntu-latest
    steps:

      - name: Checkout repository
        uses: actions/checkout@v3

      - name: Setup python
        uses: actions/setup-python@v4
        with:
          python-version: '3.12'
          
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install --upgrade pip
          pip install --upgrade wheel
          pip install -r requirements.txt
          pip install --upgrade pyautogui
          pip install --upgrade python-xlib

      - name: Install Chrome
        run: |
          sudo apt install google-chrome-stable

      - name: Execute py script --debug
        # Pass the workflow input 'url' as an environment variable named GITHUB_INPUT_URL
        env:
          GITHUB_INPUT_URL: ${{ github.event.inputs.url }}
          GITHUB_INPUT_WEBHOOK_URL: ${{ github.event.inputs.webhook_url }}
        run: |
          PROXY_ARG=""
          if [ -n "${{ github.event.inputs.proxy }}" ]; then
            PROXY_ARG="--proxy=${{ github.event.inputs.proxy }}"
          fi
          echo "DEBUG: PROXY_ARG is ->$PROXY_ARG<-"
          python sb.py --debug $PROXY_ARG
